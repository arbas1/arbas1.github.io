---
layout: post
title: "Day 26 –Using SMOTE and Feature Selection for Tumor Classification"
date: 2025-07-01
author: Arpana Basnet
permalink: /day26.html
tags: []

what_i_learned: |
  Today, I learned how to build a complete machine learning pipeline to classify brain tumor types using high-dimensional gene expression data. I explored the dataset to check its structure, cleaned it by verifying there were no missing values, and encoded the categorical labels with LabelEncoder. I practiced splitting the data into training and testing sets while keeping the class distribution balanced through stratified sampling. To prepare the features, I scaled them with MinMaxScaler, making sure to fit the scaler only on the training data and apply it separately to the test data to avoid data leakage. I also selected the top 1000 most informative features using mutual information to reduce dimensionality and help the model focus on the most relevant genes.

To address class imbalance, I applied SMOTE to generate synthetic samples and create a balanced training set. I replaced the Random Forest classifier with XGBoost, a more advanced model that performs well with large numbers of features. I learned how to configure XGBoost parameters like the number of estimators, tree depth, and learning rate to improve accuracy. After training the model on the resampled data, I evaluated it with metrics such as accuracy, precision, recall, F1-score, and a detailed classification report. This process helped me understand how careful preprocessing, feature selection, and choosing the right model can significantly improve prediction of brain tumor types in a machine learning project.
blockers: |
  Today, I faced several blockers while working on the brain tumor classification project. One major issue was that my labels were accidentally included in the feature columns, which caused errors during scaling and model training. I also had confusion about how to correctly split the data into training and testing sets and apply scaling without introducing data leakage. Initially, I used .fit_transform() on both the training and test data, which led to inconsistent scaling results. Integrating SMOTE and replacing the Random Forest classifier with XGBoost was challenging because I wasn’t sure exactly where to insert the code and how to adjust the fit and predict steps. Additionally, I ran into errors with missing imports for functions like mutual_info_classif and had trouble deciding how many top features to select to improve model accuracy without overfitting.

reflection: |
   Today, I learned how to build a machine learning pipeline to classify brain tumor types using gene expression data. I practiced encoding labels, scaling the data correctly, and selecting the top 1000 features to improve model focus. I used SMOTE to balance the training set so the model could better recognize both tumor classes. Replacing Random Forest with XGBoost taught me how powerful advanced models can be for high-dimensional datasets. Overall, this experience helped me understand the importance of careful preprocessing, model selection, and debugging to improve accuracy.
---





