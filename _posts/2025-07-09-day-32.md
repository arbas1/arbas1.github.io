---
layout: post
title: "Day 32 –Gene Expression Classification and Model Improvement"
date: 2025-07-09
author: Arpana Basnet
permalink: /day32.html
tags: []

what_i_learned: |
  Today, I worked on building a machine learning pipeline to classify brain tumor samples as benign or malignant using gene expression data. I began by selecting the top 1000 most variable genes, followed by differential expression analysis using t-tests and log2 fold change to identify significant genes. I then applied L1-regularized SVM to select the most important features and used Random Forest to further rank their importance, eventually narrowing down to the top 30 genes. With these features, I scaled the data and trained multiple models including Logistic Regression, SVM, Random Forest, XGBoost, KNN, and AdaBoost.
  
  Despite all the preprocessing and tuning, none of the models achieved more than 79% accuracy. XGBoost and AdaBoost performed the best, with decent F1 scores and ROC AUC, but still fell short of the 80% target. I explored various improvement strategies such as PCA, ensemble stacking, SMOTE for class balancing, and Recursive Feature Elimination (RFE), but the models continued to plateau below the desired accuracy.
blockers: |
  One major blocker was that despite extensive feature selection and trying multiple models, the accuracy never went beyond 79%. Even after using techniques like L1-SVM, Random Forest ranking, and PCA, the models seemed to plateau. I also noticed that some models, like SVM and Logistic Regression, gave especially low F1 scores, which pointed to possible class imbalance or overfitting. Another challenge was figuring out how many genes to keep without losing important signals or introducing too much noise. Lastly, time-consuming computations and performance limitations made it difficult to experiment with very large feature sets or more complex ensemble models like stacking.
  
reflection: |
   Today’s work helped me realize how complex and sensitive gene expression data can be when used for classification. I applied multiple well-established techniques—variance filtering, DEG analysis, L1-SVM, Random Forest ranking, and PCA—and explored a wide range of models. Even though I followed a structured, research-driven approach, the models still couldn't cross the 80% accuracy mark. This taught me that in real-world data science, results aren’t always perfect despite doing everything “right.” It also reinforced the importance of balancing performance metrics, like F1 score and AUC, rather than relying on accuracy alone. Overall, I gained valuable experience in diagnosing performance issues and thinking critically about how to improve model outcomes.
---









